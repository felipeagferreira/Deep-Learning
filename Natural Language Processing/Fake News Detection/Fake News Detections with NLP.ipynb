{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10571,
     "status": "ok",
     "timestamp": 1615606882472,
     "user": {
      "displayName": "Felipe A. Gomes Ferreira",
      "photoUrl": "",
      "userId": "09095471911889500468"
     },
     "user_tz": 180
    },
    "id": "hZj4c0xTeMwH",
    "outputId": "640db833-d0c3-4d44-9ff6-e3b235ac5399"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
      "Requirement already satisfied: huggingface-hub==0.0.2 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.7.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.2->datasets) (3.0.12)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    " !pip install datasets\n",
    " !pip install transformers\n",
    " !pip install pandas\n",
    "\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "\n",
    "try: # this is only working on the 2nd try in colab :)\n",
    "  from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "except Exception as err: # so we catch the error and import it again\n",
    "  from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "dbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfIsoKKAJ1nq"
   },
   "source": [
    "# Data Preparation\n",
    "\n",
    "## Clean the text and your targets\n",
    "Hints: \n",
    "1. Use the exploration cell to explore the data and identify cleaning steps\n",
    "2. Inspect the tokenized sentences and ensure they make sense and can leverage already trained word embeddings\n",
    "3. These resources will help you understand what type of cleaning will be required and how you can encode your text for the network:\n",
    "    - a) Preprocessing: https://huggingface.co/transformers/preprocessing.html\n",
    "    - b) Summary of tokenizers (DistilBERT uses WordPiece): https://huggingface.co/transformers/tokenizer_summary.html#wordpiece\n",
    "4. Consider the text length, is this too big/small for DistilBERT? what impact would padding/truncation have?\n",
    "5. In load data you generated a profiling report of this dataset, might be helpful to review that as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 10565,
     "status": "ok",
     "timestamp": 1615606882502,
     "user": {
      "displayName": "Felipe A. Gomes Ferreira",
      "photoUrl": "",
      "userId": "09095471911889500468"
     },
     "user_tz": 180
    },
    "id": "AT9xsN3eezI6"
   },
   "outputs": [],
   "source": [
    "def prepare_raw_data(df):\n",
    "  raw_data = df.loc[:, [\"id\", \"statement\", \"label\"]]\n",
    "  raw_data[\"label\"] = raw_data[\"label\"].astype('category')\n",
    "  return raw_data\n",
    "\n",
    "def load_data(save_dir=\"./\"):\n",
    "  dataset = load_dataset(\"liar\")\n",
    "  train = prepare_raw_data(pd.DataFrame(dataset[\"train\"]))\n",
    "  val = prepare_raw_data(pd.DataFrame(dataset[\"validation\"]))\n",
    "  test = prepare_raw_data(pd.DataFrame(dataset[\"test\"]))\n",
    "  return train, val, test\n",
    "         \n",
    "def clean_data(raw_data):\n",
    "  # TODO: What data cleaning/filtering should you consider?\n",
    "  # Hint: check for duplicates or contradictions\n",
    "  # Hint: What is the minimum and maximum lengths of the statements?\n",
    "  # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION\n",
    "\n",
    "  clean_data = raw_data.drop_duplicates(subset=[\"statement\"])\n",
    "\n",
    "\n",
    "  return clean_data\n",
    "\n",
    "def extract_raw_text_and_y(clean_data):\n",
    "  raw_text, raw_y = clean_data[\"statement\"].values, clean_data[\"label\"].values\n",
    "  raw_text = raw_text.tolist()\n",
    "  return raw_text, raw_y\n",
    "\n",
    "max_length=40\n",
    "\n",
    "def encode_text(text):\n",
    "    # TODO: encode text using dbert_tokenizer\n",
    "    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION\n",
    "                                                           \n",
    "    token_dict = dbert_tokenizer(text=text, max_length = max_length, padding='max_length', truncation=True, return_attention_mask=True)\n",
    "\n",
    "\n",
    "    input_ids = token_dict[\"input_ids\"]\n",
    "    attention_mask = token_dict[\"attention_mask\"]\n",
    "\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "\n",
    "def convert_labels(label, column):\n",
    "  if label in column:\n",
    "    return 1\n",
    "  return 0  \n",
    "\n",
    "\n",
    "def prepare_target(raw_y):\n",
    "    # TODO: convert labels to 0/1\n",
    "    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION\n",
    "    # NOTE: labels map as follows: ['false', 'half-true', 'mostly-true', 'true', 'barely-true', 'pants-fire']\n",
    "    # y should have:\n",
    "    # column 0 = \"pants-fire\" or \"false\" posts\n",
    "    # column 1 = \"true\" posts\n",
    "    # column 2 = \"pants-fire\"\n",
    "\n",
    "    y_column0 = list(map(lambda x: convert_labels(x, [0, 5]), raw_y))\n",
    "    y_column1 = list(map(lambda x: convert_labels(x, [3]), raw_y))\n",
    "    y_column2 = list(map(lambda x: convert_labels(x, [5]), raw_y))\n",
    "    y = pd.DataFrame(np.array([y_column0, y_column1, y_column2])).T\n",
    "\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2USajN2MWjn"
   },
   "source": [
    "# Modelling\n",
    "\n",
    "## Build and Train Model\n",
    "\n",
    "Resources:\n",
    "- DistilBERT paper: https://arxiv.org/abs/1910.01108\n",
    "- DistilBERT Tensorflow Documentation: https://huggingface.co/transformers/model_doc/distilbert.html#tfdistilbertmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 10554,
     "status": "ok",
     "timestamp": 1615606882509,
     "user": {
      "displayName": "Felipe A. Gomes Ferreira",
      "photoUrl": "",
      "userId": "09095471911889500468"
     },
     "user_tz": 180
    },
    "id": "GZfFboF85rIe"
   },
   "outputs": [],
   "source": [
    "def build_model(base_model, trainable=False, params={}):\n",
    "    # TODO: build the model, with the option to freeze the parameters in distilBERT\n",
    "    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION\n",
    "    # Hint 1: the cls token (token for classification in bert / distilBERT)  corresponds to the first element in the sequence in DistilBERT\n",
    "    # Hint 2: this guide may be helpful for parameter freezing: https://keras.io/guides/transfer_learning/\n",
    "    # Hint 3: double check your number of parameters make sense\n",
    "    # Hint 4: carefully consider your final layer activation and loss function\n",
    "\n",
    "    max_seq_len = max_length\n",
    "\n",
    "    # Refer to https://keras.io/api/layers/core_layers/input/\n",
    "    inputs = Input(shape = (max_seq_len,), dtype='int64', name='inputs')\n",
    "    masks  = Input(shape = (max_seq_len,), dtype='int64', name='masks')\n",
    "\n",
    "    base_model.trainable = trainable\n",
    "\n",
    "    dbert_output = base_model(inputs, attention_mask=masks)\n",
    "    dbert_last_hidden_state = dbert_output.last_hidden_state\n",
    "\n",
    "    # Any additional layers should go here\n",
    "    # use the 'params' as a dictionary for hyper parameter to facilitate experimentation\n",
    "\n",
    "    my_outputs = keras.layers.Dense(512)(dbert_last_hidden_state[:, 0, :])\n",
    "    my_outputs = keras.layers.Dropout(0.2)(my_outputs)\n",
    "    my_outputs = keras.layers.Dense(64)(my_outputs)\n",
    "    my_outputs = keras.layers.Dropout(0.2)(my_outputs)\n",
    "    probs = keras.layers.Dense(3, activation = \"sigmoid\")(my_outputs)\n",
    "\n",
    "    \n",
    "    model = keras.Model(inputs=[inputs, masks], outputs=probs)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 10543,
     "status": "ok",
     "timestamp": 1615606882512,
     "user": {
      "displayName": "Felipe A. Gomes Ferreira",
      "photoUrl": "",
      "userId": "09095471911889500468"
     },
     "user_tz": 180
    },
    "id": "xQwmE3gvCAC8"
   },
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    # TODO: compile the model, include relevant auc metrics when training\n",
    "    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION\n",
    "    # Hint: you may want to read up on the \"multi_label\" parameter in the keras AUC metrics\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", \n",
    "                  metrics = [keras.metrics.AUC(), keras.metrics.AUC(curve=\"PR\"), keras.metrics.Recall(), keras.metrics.Precision()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 10535,
     "status": "ok",
     "timestamp": 1615606882515,
     "user": {
      "displayName": "Felipe A. Gomes Ferreira",
      "photoUrl": "",
      "userId": "09095471911889500468"
     },
     "user_tz": 180
    },
    "id": "j9Gc9ajwC3UO"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "\n",
    "def train_model(model, model_inputs_and_masks_train, model_inputs_and_masks_val,\n",
    "    y_train, y_val, batch_size, num_epochs):\n",
    "    # TODO: train the model\n",
    "    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION\n",
    "\n",
    "    history = model.fit([np.array(train_model_inputs_and_masks['inputs']), np.array(train_model_inputs_and_masks['masks'])],\n",
    "                        y_train, batch_size = batch_size, epochs = num_epochs,\n",
    "                        validation_data = ([np.array(model_inputs_and_masks_val['inputs']), np.array(model_inputs_and_masks_val['masks'])], y_val)\n",
    "                        )\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 10525,
     "status": "ok",
     "timestamp": 1615606882518,
     "user": {
      "displayName": "Felipe A. Gomes Ferreira",
      "photoUrl": "",
      "userId": "09095471911889500468"
     },
     "user_tz": 180
    },
    "id": "vkeGty1ZGccC"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_inputs_and_masks_test, y_test):\n",
    "    # TODO: evaluate the model\n",
    "    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION\n",
    "    # HINT: for pr_auc: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html \n",
    "\n",
    "    eval_dict = model.evaluate([np.array(model_inputs_and_masks_test['inputs']),\n",
    "                                np.array(model_inputs_and_masks_test['masks'])], np.array(y_test), verbose = 2)\n",
    "    \n",
    "    \n",
    "\n",
    "   # eval_dict = {\n",
    "   #     \"false\": {\n",
    "   #         \"pr_auc\": ???, \"pr_auc_random_guess\": ???, \n",
    "   #         \"roc_auc\": ???, \"roc_auc_random_guess\": ???, \n",
    "   #         \"precision\": ???, \"recall\": ???\n",
    "   #     }, \n",
    "   #     \"true\": {\n",
    "   #         \"pr_auc\": ???, \"pr_auc_random_guess\": ???, \n",
    "   #         \"roc_auc\": ???, \"roc_auc_random_guess\": ???, \n",
    "   #         \"precision\": ???, \"recall\": ???\n",
    "   #     }, \n",
    "   #     \"pants\": {\n",
    "   #        \"pr_auc\": ???, \"pr_auc_random_guess\": ???, \n",
    "   #         \"roc_auc\": ???, \"roc_auc_random_guess\": ???, \n",
    "   #         \"precision\": ???, \"recall\": ???\n",
    "   #     }\n",
    "   # }\n",
    "    return eval_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qg_oKZr5VoCU"
   },
   "source": [
    "# Execution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20289,
     "status": "ok",
     "timestamp": 1615606892304,
     "user": {
      "displayName": "Felipe A. Gomes Ferreira",
      "photoUrl": "",
      "userId": "09095471911889500468"
     },
     "user_tz": 180
    },
    "id": "0xEWJaqCMTCB",
    "outputId": "cb3d58b7-d966-419a-a162-f74e3e6f19a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset liar (/root/.cache/huggingface/datasets/liar/default/1.0.0/1a6abd9863f27194da30fcb66988477abfa3780df3b0ad1d0032979c48ec7918)\n"
     ]
    }
   ],
   "source": [
    "## DO NOT Change\n",
    "train, val, test = load_data()\n",
    "train_raw_x, train_raw_y = extract_raw_text_and_y(clean_data(train))\n",
    "val_raw_x, val_raw_y = extract_raw_text_and_y(clean_data(val))\n",
    "test_raw_x, test_raw_y = extract_raw_text_and_y(clean_data(test))\n",
    "\n",
    "train_input, train_mask = encode_text(train_raw_x)\n",
    "train_y = prepare_target(train_raw_y)\n",
    "\n",
    "val_input, val_mask = encode_text(val_raw_x)\n",
    "val_y = prepare_target(val_raw_y)\n",
    "\n",
    "test_input, test_mask = encode_text(test_raw_x)\n",
    "test_y = prepare_target(test_raw_y)\n",
    "\n",
    "train_model_inputs_and_masks = {\n",
    "    'inputs' : train_input,\n",
    "    'masks' : train_mask\n",
    "}\n",
    "\n",
    "val_model_inputs_and_masks = {\n",
    "    'inputs' : val_input,\n",
    "    'masks' : val_mask\n",
    "}\n",
    "\n",
    "test_model_inputs_and_masks = {\n",
    "    'inputs' : test_input,\n",
    "    'masks' : test_mask\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e6WD5qyWkcm"
   },
   "source": [
    "\n",
    "Use the cell below to execute and experiment with your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2376496,
     "status": "ok",
     "timestamp": 1615609248528,
     "user": {
      "displayName": "Felipe A. Gomes Ferreira",
      "photoUrl": "",
      "userId": "09095471911889500468"
     },
     "user_tz": 180
    },
    "id": "eYjCiyY-WiNY",
    "outputId": "41dfe606-9923-47e7-ea5b-418972c42199"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'activation_13', 'vocab_layer_norm', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masks (InputLayer)              [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_1 (TFDisti TFBaseModelOutput(la 66362880    inputs[0][0]                     \n",
      "                                                                 masks[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_1 (Sli (None, 768)          0           tf_distil_bert_model_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          393728      tf.__operators__.getitem_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 512)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           32832       dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 64)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 3)            195         dropout_41[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 66,789,635\n",
      "Trainable params: 426,755\n",
      "Non-trainable params: 66,362,880\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.5300 - auc_2: 0.6285 - auc_3: 0.2344 - recall_1: 0.1148 - precision_1: 0.2698WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "161/161 [==============================] - 772s 5s/step - loss: 0.5297 - auc_2: 0.6286 - auc_3: 0.2345 - recall_1: 0.1147 - precision_1: 0.2700 - val_loss: 0.4358 - val_auc_2: 0.7136 - val_auc_3: 0.3423 - val_recall_1: 0.0843 - val_precision_1: 0.5138\n",
      "Epoch 2/3\n",
      "161/161 [==============================] - 751s 5s/step - loss: 0.4527 - auc_2: 0.6661 - auc_3: 0.2875 - recall_1: 0.0834 - precision_1: 0.3757 - val_loss: 0.4187 - val_auc_2: 0.7260 - val_auc_3: 0.3626 - val_recall_1: 0.0211 - val_precision_1: 0.5385\n",
      "Epoch 3/3\n",
      "161/161 [==============================] - 745s 5s/step - loss: 0.4421 - auc_2: 0.6813 - auc_3: 0.2972 - recall_1: 0.0732 - precision_1: 0.3860 - val_loss: 0.4138 - val_auc_2: 0.7327 - val_auc_3: 0.3652 - val_recall_1: 0.0181 - val_precision_1: 0.6000\n",
      "41/41 - 81s - loss: 0.4083 - auc_2: 0.7293 - auc_3: 0.3527 - recall_1: 0.0310 - precision_1: 0.6061\n"
     ]
    }
   ],
   "source": [
    "dbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "model = build_model(dbert_model, params={})\n",
    "model = compile_model(model)\n",
    "model, history = train_model(model, train_model_inputs_and_masks, val_model_inputs_and_masks, train_y, val_y, batch_size, num_epochs)\n",
    "eval_dict = evaluate_model(model, test_model_inputs_and_masks, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_KZbODkaHu_"
   },
   "source": [
    "## Conclusions (TODO)\n",
    "TODO: Make Your Final Conclusions About Your Model (Answer questions below, answer in this cell)\n",
    "- a) What is driving your model's decisions?\n",
    "- b) Is your model biased in some ways? If so how? \n",
    "- c) Does your model accomplish the objectives? If not, is your model useful and how can you justify this?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Cópia2 de 20244867_Ferreira_Felipe_ex3.ipynb",
   "provenance": [
    {
     "file_id": "1iuWVkm8synrR7SaOSk4YfvNIP91jhpkK",
     "timestamp": 1615600565144
    },
    {
     "file_id": "1n9BUkmJEI7bTuFFzqmyvWsK2A5-cP_Yf",
     "timestamp": 1615594274124
    },
    {
     "file_id": "1ZwtvQp4tLOrgcL3qs95PH6hw6whnjcVc",
     "timestamp": 1615402549952
    },
    {
     "file_id": "19HwcejHEIfWg1nzD-81D_hBzHAqG50rZ",
     "timestamp": 1613453034431
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
